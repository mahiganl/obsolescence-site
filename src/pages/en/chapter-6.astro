---
import BaseLayout from '../../layouts/BaseLayout.astro';
import ChapterNavigation from '../../components/ChapterNavigation.astro';
import ChapterIllustration from '../../components/ChapterIllustration.astro';

const lang = 'en';
const currentSlug = 'chapter-6';

import chapterImage from '../../../assets/colored-drawings/chapter 6.png';
---

<BaseLayout title="Chapter 6 – The Risk of Extinction" lang={lang} currentSlug={currentSlug}>
  <!-- Part Header -->
  <div class="text-center mb-8">
    <p class="text-4xl sm:text-5xl font-sans font-bold uppercase tracking-wider text-stone-600 dark:text-stone-300">
      PART III
    </p>
    <p class="text-2xl sm:text-3xl font-sans font-bold uppercase tracking-wider text-stone-600 dark:text-stone-300 mt-2">
      THE EXISTENTIAL RISK
    </p>
  </div>

  <ChapterIllustration src={chapterImage.src} alt="Chapter 6 illustration" lang={lang} />

  <header class="mb-10 text-center">
    <p class="text-sm font-sans uppercase tracking-wider text-stone-500 dark:text-stone-400 mb-2">Chapter 6</p>
    <h1 class="text-4xl sm:text-5xl font-bold text-stone-900 dark:text-stone-100 leading-tight">
      The Risk of Extinction
    </h1>
  </header>

  <article class="prose mx-auto">
    <p>The economic scenario described above is, in my view, the most likely outcome—provided we let current trends continue without the technical and legislative guardrails needed to distribute the power and wealth generated by AGI.</p>

    <p>Likely, that is, assuming we are still here to experience it.</p>

    <p>To those new to AI safety, this possibility may seem extreme or alarmist. Yet, it is a scenario considered plausible by Yoshua Bengio, Geoffrey Hinton, Elon Musk (xAI, Grok), and Sam Altman (OpenAI, ChatGPT). All have stated that AI poses an existential risk to humanity.</p>

    <p>Musk frequently cites a 10 to 20% probability of catastrophe. However, he maintains that if xAI does not develop an AI aligned with the "truth"—specifically <em>his</em> highly contestable version of it—less scrupulous actors will win the race. We saw a glimpse of this "truth" last year when Grok launched into far-right tirades and christened itself <em>Mecha-Hitler</em>...</p>

    <p>As for Altman, despite founding OpenAI with a safety mission, his credibility has been tarnished by the exodus of key researchers from his "Superalignment" division. Figures like co-founder Ilya Sutskever, Jan Leike, and Daniel Kokotajlo resigned, condemning a corporate culture that prioritized commercial products over safety<a id="ref7" href="#note7"><sup>7</sup></a>.</p>

    <p>Yet, in May 2023, Altman signed a statement from the Center for AI Safety (CAIS) alongside Bengio, Hinton, Demis Hassabis (Google DeepMind), and Dario Amodei (Anthropic). It asserted that "mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"<a id="ref8" href="#note8"><sup>8</sup></a>.</p>

    <p>It is hard to imagine the Sam Altman of today discussing an "extinction risk from AI," but the record stands. What came of that 2023 declaration? Essentially nothing. But it proves, at the very least, that concern over extinction isn't limited to the fringe theories of so-called "doomers."</p>

    <p>In fact, surveys show that many researchers assign a significant probability to a doomsday scenario—known as <em>P(doom)</em>. In the largest study to date (involving 2,778 experts), the median probability of AI causing human extinction is about 5%. More than a third of researchers (38%) place this risk at 10% or higher<a id="ref9" href="#note9"><sup>9</sup></a>.</p>

    <p>To put these numbers in perspective: in the nuclear industry, a catastrophe risk (like Chernobyl or Fukushima) greater than 0.0001% is considered unacceptable.</p>

    <hr />

    <p class="text-sm text-stone-500 dark:text-stone-400">
      <a id="note7" href="#ref7"><sup>7</sup></a> Kokotajlo subsequently co-authored a prospective scenario titled <em>AI 2027</em> (<a href="https://ai-2027.com" target="_blank" rel="noopener noreferrer">ai-2027.com</a>), which details the mechanisms leading to a potential loss of control—and the safeguards that could prevent it. Initially published in April 2025, <em>AI 2027</em> anticipated several developments that are now unfolding: the US-China race, the focus on automating programming work, AI companies' stated goal of automating AI research (recursion) confirmed by recent announcements, the acceleration of robotics, etc.<br />
      <a id="note8" href="#ref8"><sup>8</sup></a> "Statement on AI Risk", open letter, Center for AI Safety (<a href="https://aistatement.com/" target="_blank" rel="noopener noreferrer">aistatement.com</a>).<br />
      <a id="note9" href="#ref9"><sup>9</sup></a> Katja Grace <em>et al.</em>, "Thousands of AI Authors on the Future of AI", <em>Journal of Artificial Intelligence Research</em> 84:9, 2025. <a href="https://arxiv.org/abs/2401.02843" target="_blank" rel="noopener noreferrer">arxiv.org/abs/2401.02843</a>
    </p>
  </article>

  <ChapterNavigation lang={lang} currentSlug={currentSlug} />
</BaseLayout>
