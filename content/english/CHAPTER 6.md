# THE RISK OF EXTINCTION

The economic scenario described above is, in my view, the most likely outcome—provided we let current trends continue without the technical and legislative guardrails needed to distribute the power and wealth generated by AGI.

Likely, that is, assuming we are still here to experience it.

To those new to AI safety, this possibility may seem extreme or alarmist. Yet, it is a scenario considered plausible by Yoshua Bengio, Geoffrey Hinton, Elon Musk (xAI, Grok), and Sam Altman (OpenAI, ChatGPT). All have stated that AI poses an existential risk to humanity.

Musk frequently cites a 10 to 20% probability of catastrophe. However, he maintains that if xAI does not develop an AI aligned with the "truth"—specifically _his_ highly contestable version of it—less scrupulous actors will win the race. We saw a glimpse of this "truth" last year when Grok launched into far-right tirades and christened itself _Mecha-Hitler_...

As for Altman, despite founding OpenAI with a safety mission, his credibility has been tarnished by the exodus of key researchers from his "Superalignment" division. Figures like co-founder Ilya Sutskever, Jan Leike, and Daniel Kokotajlo resigned, condemning a corporate culture that prioritized commercial products over safety[^7].

Yet, in May 2023, Altman signed a statement from the Center for AI Safety (CAIS) alongside Bengio, Hinton, Demis Hassabis (Google DeepMind), and Dario Amodei (Anthropic). It asserted that "mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"[^8].

It is hard to imagine the Sam Altman of today discussing an "extinction risk from AI," but the record stands. What came of that 2023 declaration? Essentially nothing. But it proves, at the very least, that concern over extinction isn't limited to the fringe theories of so-called "doomers."

In fact, surveys show that many researchers assign a significant probability to a doomsday scenario—known as _P(doom)_. In the largest study to date (involving 2,778 experts), the median probability of AI causing human extinction is about 5%. More than a third of researchers (38%) place this risk at 10% or higher[^9].

To put these numbers in perspective: in the nuclear industry, a catastrophe risk (like Chernobyl or Fukushima) greater than 0.0001% is considered unacceptable.

[^7]: Kokotajlo subsequently co-authored a prospective scenario titled _AI 2027_ (ai-2027.com), which details the mechanisms leading to a potential loss of control—and the checks that would allow it to be avoided. Initially published in April 2025, _AI 2027_ anticipated several elements now being confirmed in reality: the China-US race, the focus on automating programming work, the desire of AI companies to automate AI research (recursivity) confirmed by recent statements, the acceleration of developments in robotics, etc. 
[^8]: "Statement on AI Risk", open letter, Center for AI Safety, [https://aistatement.com/](https://aistatement.com/) (accessed December 20, 2025).
[^9]: Katja Grace _et al._, "Thousands of AI Authors on the Future of AI", _Journal of Artificial Intelligence Research_ 84:9, 2025, [https://arxiv.org/abs/2401.02843](https://arxiv.org/abs/2401.02843) (accessed December 19, 2025).