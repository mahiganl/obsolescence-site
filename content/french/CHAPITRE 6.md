# LE RISQUE D’EXTINCTION

Le scénario économique que je viens de décrire est selon moi le plus probable si nous laissons les choses suivre leur cours sans mettre en place les remparts techniques et législatifs qui permettraient de mieux distribuer le pouvoir et la richesse qui seront générés par l’intelligence artificielle générale.

Probable, à une condition près : que nous soyons encore là pour en faire l’expérience.

Pour ceux et celles qui ne sont pas familiers avec le domaine de la sûreté de l’IA, cette possibilité peut paraître extrême ou alarmiste. Pourtant, c’est un scénario qui ne semble improbable ni pour Yoshua Bengio ni pour Geoffrey Hinton, ni pour Elon Musk (xAI, Grok) ni pour Sam Altman (OpenAI, ChatGPT). Toutes ces personnes ont déclaré que l’IA posait un risque existentiel pour l’humanité. Musk répète souvent qu’il existe une probabilité de 10 à 20 % de catastrophe, mais soutient que si sa société xAI ne développe pas une IA alignée sur la « vérité » – c’est-à-dire *sa* vérité, très contestable au vu des positions qu’il tente d’inculquer à Grok, qui l’an dernier s’est lancé dans des tirades d’extrême-droite en s’auto-baptisant *Mecha-Hitler* –, d’autres acteurs moins scrupuleux gagneront la course. Quant à Altman, bien qu’il ait fondé OpenAI avec une mission de sûreté, sa crédibilité est aujourd’hui entachée par le départ massif de chercheurs clés de sa division « Superalignment ». Ces derniers, dont le cofondateur Ilya Sutskever, Jan Leike ou encore Daniel Kokotajlo, ont démissionné en dénonçant une culture d'entreprise privilégiant les produits commerciaux au détriment de la sûreté[^7].

Reste qu’en mai 2023, Altman a signé – aux côtés de Bengio, Hinton, Demis Hassabis (PDG de Google DeepMind) et Dario Amodei (ancien vice-président de la recherche chez OpenAI et fondateur d’Anthropic) – une déclaration du Center for AI Safety (CAIS) affirmant que « l’atténuation du risque d'extinction lié à l'IA devrait être une priorité mondiale au même titre que d'autres risques à l'échelle de la société, tels que les pandémies et la guerre nucléaire »[^8].

On n’imagine plus le Sam Altman de 2025 ou 2026 parler du « risque d’extinction lié à l’IA », mais les écrits restent. Quelle suite a eu la déclaration de 2023? En substance, aucune. Mais elle montre au moins que la préoccupation en matière d'extinction causée par l’IA ne vient pas seulement des discours marginaux de ceux que l’on appelle les *doomers* – les prophètes d’apocalypse.

En fait, lorsque l’on sonde les chercheurs en IA, on se rend compte que bon nombre d’entre eux ont un *P(doom)* — une prédiction probabiliste de scénario apocalyptique — significatif. Selon la plus vaste étude menée à ce jour auprès de 2 778 experts, la probabilité médiane de voir l'IA causer l'extinction de notre espèce est d'environ 5 %, et plus d'un tiers des chercheurs (38 %) placent ce risque à 10 % ou plus[^9].

Pour mettre ces chiffres en perspective, dans le domaine du nucléaire, on juge inacceptable un risque de catastrophe – du genre Tchernobyl ou Fukushima – supérieur à 0,0001 %.

[^7]: Kokotajlo a ensuite co-rédigé un scénario prospectif intitulé *AI 2027* (ai-2027.com), qui décrit dans le détail les mécanismes menant à une éventuelle perte de contrôle – et les freins qui permettraient au contraire de l’éviter. Publié initialement en avril 2025, *AI 2027* a anticipé plusieurs éléments qui se confirment dans le réel aujourd’hui : la course Chine-États-Unis, la focalisation sur l’automatisation du travail de programmation, la volonté des sociétés d’IA d’automatiser la recherche en IA (récursivité) confirmée par des déclarations récentes, l’accélération des développements en robotique, etc.
[^8]: « Statement on AI Risk », lettre ouverte, Center for AI Safety, https://aistatement.com/ (page consultée le 20 décembre 2025); ma traduction.
[^9]: Katja Grace *et al.*, « Thousands of AI Authors on the Future of AI », *Journal of Artificial Intelligence Research* 84:9, 2025, https://arxiv.org/abs/2401.02843 (page consultée le 19 décembre 2025).