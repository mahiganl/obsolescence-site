# 10. L’EXPLOSION DE L’INTELLIGENCE

Le scénario du grand déclassement et celui de l’extinction ne sont pas forcément exclusifs : ils peuvent très bien advenir de façon subséquente. Au fur et à mesure que l’on s’approche de l’IAG et une fois qu’on y est, la vaste majorité des humains fait l’expérience de son obsolescence et de son déclin économique. Puis, une fois que l’IAG cède le pas à la superintelligence, le scénario de l’extinction s’enclenche.

La disparition de l’espèce humaine pourrait aussi advenir des années après l’avènement de la SIA, si celle-ci juge qu’il vaut mieux garder ses cartes dans sa manche, manipuler les humains tranquillement, nouer des liens d’attachement avec eux (c’est déjà commencé : plusieurs considèrent leur *chatbot* comme leur ami.e ou même leur ami.e de cœur), attendre patiemment les progrès de la robotique, etc.

Combien de temps s’écoulera-t-il entre l’IAG et la SIA? On ne peut le savoir avec certitude, mais selon plusieurs, la transition pourrait bien ne durer que l’espace d’un instant. C’est l’hypothèse du *foom*, selon le bruit que fait une fusée en s’envolant. Une fois que la machine remplacera les chercheurs en IA, induisant une boucle d’auto-amélioration que l’on appelle « récursivité », le processus risque de s’emballer et l’on pourrait assister à ce que I.J. Good a appelé, en 1965, une « explosion de l’intelligence ». À partir de là, difficile de dire ce qui va se passer.

On appelle cette situation « la singularité », un terme emprunté à la physique des trous noirs. Dans cet état, les choses deviennent étranges, très différentes et imprévisibles, et la rapidité des progrès dépasse l’entendement humain. On ne peut pas observer ce qu’il y a au-delà de l’horizon du *foom*.

Chose certaine, une fois la singularité advenue, nous (les êtres humains) ne contrôlerons plus rien. C’est pourquoi il est si important de s’assurer de la sûreté de l’IA *avant* le décollage. Or, en ce moment, on assiste à un sprint vers l’intelligence générale et la superintelligence, une véritable course aux armements entre les États-Unis et la Chine, d’une part, et entre les différentes sociétés de pointe, de l’autre. La SIA, en effet, sera une arme : l’instrument de pouvoir le plus puissant jamais créé sur Terre, dépassant de loin le nucléaire. Cette course fait en sorte que les acteurs investissent sans compter dans le renforcement de la capacité brute et infiniment moins dans la sûreté de l’IA. Si nous ne corrigeons pas rapidement la trajectoire, nous courons vers la catastrophe.

Il y a en ce moment un écart immense entre la réalité du risque et l’attitude des entreprises d’IA et des décideurs. Comme interprète de conférence, je travaille souvent pour la Commission économique et sociale pour l'Asie et le Pacifique. On y discute de durabilité, de prospérité, de changements climatiques – des enjeux fondamentaux –, mais lorsque l’on parle d’IA, c’est presque toujours pour faire la promotion d’initiatives nationales (*AI Hubs*, intégration de l’IA dans la gouvernance, etc.), rarement pour alerter sur les dangers. À l’échelle mondiale, l’ONU a créé un Organe consultatif sur l’IA et adopté le Pacte numérique mondial, mais ces avancées diplomatiques paraissent dérisoires face à l’urgence. Certains experts croient secrètement qu’il faudra un coup de semonce (*warning shot*) – un incident sécuritaire majeur impliquant des pertes humaines massives – pour sortir l’opinion publique et les gouvernements de leur torpeur. J’espère que nous nous réveillerons avant cela.

Pour que la discussion ait lieu dans les hautes sphères, il faut d’abord qu’elle commence dans les foyers, dans la rue, dans les médias. On ne doit pas se laisser intimider par le sujet. Rappelez-vous : les LLM sont des boîtes noires, les chercheurs en IA ne savent pas plus que nous (ou à peine) ce qui se passe là-dedans.

La conversation appartient à tous. Ne laissons pas quelques PDG de la Silicon Valley ou quelques dirigeants chinois décider de notre avenir et de celui de nos enfants et petits-enfants. Il est temps que nous prenions la parole collectivement.